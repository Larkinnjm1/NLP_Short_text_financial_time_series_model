{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#NMF-topic-modelling-for-short-text\" data-toc-modified-id=\"NMF-topic-modelling-for-short-text-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>NMF topic modelling for short text</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF topic modelling for short text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import numpy as np\n",
    "import argparse\n",
    "from utils import *\n",
    "import easydict\n",
    "import re\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from model import *\n",
    "os.chdir(r'C:\\Users\\niall\\Dropbox\\Data science masters\\IN3061 INM430 Prin of Data Sc\\Coursework\\Documentation\\SeaNMF-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '2009jan_2_jun_topics_no_text_preprocessing.xlsx',\n",
       " 'data',\n",
       " 'data_process.py',\n",
       " 'LICENSE',\n",
       " 'model.py',\n",
       " 'NMF Short text topic modelling.ipynb',\n",
       " 'README.md',\n",
       " 'seanmf_results',\n",
       " 'train.py',\n",
       " 'utils.py',\n",
       " 'vis_topic.py',\n",
       " '__pycache__',\n",
       " '~$2009jan_2_jun_topics_no_text_preprocessing.xlsx']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_Pre_process(text_file_loc,corpus_file_loc,vocab_file_loc,vocab_max_size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Visualize Topics\n",
    "    '''\n",
    "\n",
    "\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--text_file', default='data/data.txt', help='input text file')\n",
    "    #parser.add_argument('--corpus_file', default='data/doc_term_mat.txt', help='term document matrix file')\n",
    "    #parser.add_argument('--vocab_file', default='data/vocab.txt', help='vocab file')\n",
    "    #parser.add_argument('--vocab_max_size', type=int, default=10000, help='maximum vocabulary size')\n",
    "    #parser.add_argument('--vocab_min_count', type=int, default=3, help='minimum frequency of the words')\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "\n",
    "    import easydict\n",
    "\n",
    "    args=easydict.EasyDict({'text_file':text_file_loc,#'data/2009_jan2Jun_abc_news_text_lemma_notepad.txt', #Parameters that are inputted into thte model prior to starting processing\n",
    "                            'corpus_file':corpus_file_loc,#'data/doc_term_mat_abcnews.txt',\n",
    "                            'vocab_file':vocab_file_loc,#'data/vocab.txt',\n",
    "                            'vocab_max_size':vocab_max_size,#10000,\n",
    "                            'vocab_min_count':3\n",
    "                            })\n",
    "\n",
    "    # create vocabulary\n",
    "    print('create vocab')\n",
    "    vocab = {}\n",
    "    fp = open(args.text_file, 'r')\n",
    "    for line in fp:\n",
    "        arr = re.split('\\s', line[:-1]) \n",
    "        for wd in arr:\n",
    "            try:\n",
    "                vocab[wd] += 1\n",
    "            except:\n",
    "                vocab[wd] = 1 #Creating complete vocabulary list \n",
    "    fp.close()\n",
    "    vocab_arr = [[wd, vocab[wd]] for wd in vocab if vocab[wd] > args.vocab_min_count] #Creating a dictionary from text file \n",
    "    vocab_arr = sorted(vocab_arr, key=lambda k: k[1])[::-1]\n",
    "    vocab_arr = vocab_arr[:args.vocab_max_size]\n",
    "    vocab_arr = sorted(vocab_arr)\n",
    "\n",
    "    fout = open(args.vocab_file, 'w') #writing dictionary to vocab text file\n",
    "    for itm in vocab_arr:\n",
    "        itm[1] = str(itm[1])\n",
    "        fout.write(' '.join(itm)+'\\n')\n",
    "    fout.close()\n",
    "\n",
    "    # vocabulary to id\n",
    "    vocab2id = {itm[1][0]: itm[0] for itm in enumerate(vocab_arr)}\n",
    "    print('create document term matrix')\n",
    "    data_arr = []\n",
    "    fp = open(args.text_file, 'r') #Opens up text file and is generating a document term matrix\n",
    "    fout = open(args.corpus_file, 'w')\n",
    "    for line in fp: \n",
    "        arr = re.split('\\s', line[:-1]) \n",
    "        arr = [str(vocab2id[wd]) for wd in arr if wd in vocab2id]\n",
    "        sen = ' '.join(arr)\n",
    "        fout.write(sen+'\\n')\n",
    "    fp.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create vocab\n",
      "create document term matrix\n"
     ]
    }
   ],
   "source": [
    "NMF_Pre_process('data/2009_jan2Jun_abc_news_text_lemma_notepad.txt','data/doc_term_mat_abcnews.txt','data/vocab.txt',10000)\n",
    "\n",
    " #   args=easydict.EasyDict({'text_file':text_file_loc,#'data/2009_jan2Jun_abc_news_text_lemma_notepad.txt', #Parameters that are inputted into thte model prior to starting processing\n",
    "  #                          'corpus_file':corpus_file_loc,#'data/doc_term_mat_abcnews.txt',\n",
    "   #                         'vocab_file':vocab_file_loc,#'data/vocab.txt',\n",
    "    #                        'vocab_max_size':vocab_max_size,#10000,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_training_model(corpus_file_loc,vocab_file_loc,max_iter,n_topics):\n",
    "    \n",
    "    '''\n",
    "    SeaNMF Training\n",
    "    '''\n",
    "\n",
    "\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--corpus_file', default='data/doc_term_mat.txt', help='term document matrix file')\n",
    "    #parser.add_argument('--vocab_file', default='data/vocab.txt', help='vocab file')\n",
    "    #parser.add_argument('--model', default='seanmf', help='nmf | seanmf')\n",
    "    #parser.add_argument('--max_iter', type=int, default=500, help='max number of iterations')\n",
    "    #parser.add_argument('--n_topics', type=int, default=100, help='number of topics')\n",
    "    #parser.add_argument('--alpha', type=float, default=0.1, help='alpha')\n",
    "    #parser.add_argument('--beta', type=float, default=0.0, help='beta')\n",
    "    #parser.add_argument('--max_err', type=float, default=0.1, help='stop criterion')\n",
    "    #parser.add_argument('--fix_seed', type=bool, default=True, help='set random seed 0')\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    args=easydict.EasyDict({'corpus_file':corpus_file_loc, #'data/doc_term_mat_abcnews.txt', #Loading model attributes into a single file \n",
    "                            'vocab_file':vocab_file_loc,#'data/vocab.txt',\n",
    "                            'model':'seanmf',\n",
    "                            'max_iter':max_iter,#100,\n",
    "                            'n_topics':n_topics,#300,\n",
    "                            'alpha':0.1,\n",
    "                            'beta':0.0,\n",
    "                            'max_err':0.1,\n",
    "                            'fix_seed':True\n",
    "                            })\n",
    "\n",
    "    docs = read_docs(args.corpus_file) #Reading in text corpus file \n",
    "    vocab = read_vocab(args.vocab_file) #Reading in vocabulary file\n",
    "    n_docs = len(docs) #Determining the total number of documents \n",
    "    n_terms = len(vocab) #Determining the total number of terms\n",
    "    print('n_docs={}, n_terms={}'.format(n_docs, n_terms))\n",
    "\n",
    "    tmp_folder = 'seanmf_results'\n",
    "    if not os.access(tmp_folder, os.F_OK): #accesining folder seanmf_results to store final results in text format in\n",
    "        os.mkdir(tmp_folder)\n",
    "\n",
    "    if args.model.lower() == 'nmf': \n",
    "        print('read term doc matrix')\n",
    "        dt_mat = np.zeros([n_terms, n_docs]) #initialising empty term document matrix to be filled during processing\n",
    "        for k in range(n_docs):\n",
    "            for j in docs[k]: # filling counts of each word in term document matrix \n",
    "                dt_mat[j, k] += 1.0\n",
    "        print('term doc matrix done')\n",
    "        print('-'*50)\n",
    "\n",
    "        model = NMF( #generating NMF model with parameters term document matrix, max error, max number of toics and max number of iterations\n",
    "            dt_mat, \n",
    "            n_topic=args.n_topics, \n",
    "            max_iter=args.max_iter, \n",
    "            max_err=args.max_err)\n",
    "\n",
    "        model.save_format( #Save model in temporary folder\n",
    "            Wfile=tmp_folder+'/W.txt',\n",
    "            Hfile=tmp_folder+'/H.txt')\n",
    "\n",
    "    if args.model.lower() == 'seanmf': #Generate co-occurance matrix \n",
    "        print('calculate co-occurance matrix')\n",
    "        dt_mat = np.zeros([n_terms, n_terms]) #generate empty co-occurence to store data into. \n",
    "        for itm in docs:\n",
    "            for kk in itm:\n",
    "                for jj in itm:\n",
    "                    dt_mat[int(kk),int(jj)] += 1.0 #Adding value each time cooccurence is denoted between words \n",
    "        print('co-occur done')\n",
    "        print('-'*50)\n",
    "        print('calculate PPMI')\n",
    "        D1 = np.sum(dt_mat) #PMI value calculated between words in topic to determine the give an indicator of cooccurence of these values \n",
    "        SS = D1*dt_mat\n",
    "        for k in range(n_terms):\n",
    "            SS[k] /= np.sum(dt_mat[k])\n",
    "        for k in range(n_terms):\n",
    "            SS[:,k] /= np.sum(dt_mat[:,k])\n",
    "        dt_mat = [] # release memory\n",
    "        SS[SS==0] = 1.0\n",
    "        SS = np.log(SS)\n",
    "        SS[SS<0.0] = 0.0\n",
    "        print('PPMI done')\n",
    "        print('-'*50)\n",
    "\n",
    "        print('read term doc matrix') #Size of term document matrix primary cause for memory error please ensure text corpus<1800kB\n",
    "        dt_mat = np.zeros([n_terms, n_docs])\n",
    "        for k in range(n_docs):\n",
    "            for j in docs[k]: #Fill in term document matrix into a numpy array \n",
    "                dt_mat[j, k] += 1.0\n",
    "        print('term doc matrix done')\n",
    "        print('-'*50)\n",
    "\n",
    "        model = SeaNMFL1(\n",
    "            dt_mat, SS,  \n",
    "            alpha=args.alpha, \n",
    "            beta=args.beta, \n",
    "            n_topic=args.n_topics, \n",
    "            max_iter=args.max_iter, \n",
    "            max_err=args.max_err,\n",
    "            fix_seed=args.fix_seed)\n",
    "\n",
    "        model.save_format(\n",
    "            W1file=tmp_folder+'/W.txt',\n",
    "            W2file=tmp_folder+'/Wc.txt',\n",
    "            Hfile=tmp_folder+'/H.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read documents\n",
      "--------------------------------------------------\n",
      "read vocabulary\n",
      "--------------------------------------------------\n",
      "n_docs=37996, n_terms=5999\n",
      "calculate co-occurance matrix\n",
      "co-occur done\n",
      "--------------------------------------------------\n",
      "calculate PPMI\n",
      "PPMI done\n",
      "--------------------------------------------------\n",
      "read term doc matrix\n",
      "term doc matrix done\n",
      "--------------------------------------------------\n",
      "loop begin\n",
      "Step=0, Loss=317385.16532171227, Time=20.73029851913452s\n",
      "Step=1, Loss=315124.3182260943, Time=33.059083700180054s\n",
      "Step=2, Loss=313561.04908366886, Time=41.877951860427856s\n",
      "Step=3, Loss=311723.41994781, Time=49.32042217254639s\n",
      "Step=4, Loss=310804.23762175057, Time=57.22863554954529s\n",
      "Step=5, Loss=310392.52084015845, Time=66.21713638305664s\n",
      "Step=6, Loss=310198.67819562356, Time=72.68447279930115s\n",
      "Step=7, Loss=310106.3264020791, Time=79.13097739219666s\n",
      "Step=8, Loss=310056.1336465711, Time=86.56402373313904s\n",
      "Step=9, Loss=310024.3543219692, Time=92.47931170463562s\n"
     ]
    }
   ],
   "source": [
    "NMF_training_model('data/doc_term_mat_abcnews.txt','data/vocab.txt',10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_Printout_topics(corpus_file_loc,vocab_file_loc,par_file_loc,results_file_name):\n",
    "     \n",
    "    '''\n",
    "    Visualize Topics\n",
    "    '''\n",
    "\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--corpus_file', default='data/doc_term_mat.txt', help='term document matrix file')\n",
    "    #parser.add_argument('--vocab_file', default='data/vocab.txt', help='vocab file')\n",
    "    #parser.add_argument('--par_file', default='seanmf_results/W.txt', help='model results file')\n",
    "    #opt = parser.parse_args()\n",
    "\n",
    "    opt=easydict.EasyDict({'corpus_file':corpus_file_loc,#'data/doc_term_mat_abcnews.txt',\n",
    "                            'vocab_file':vocab_file_loc,#'data/vocab.txt',\n",
    "                            'par_file':par_file_loc,#'seanmf_results/W.txt',\n",
    "                            })\n",
    "\n",
    "    docs = read_docs(opt.corpus_file)\n",
    "    vocab = read_vocab(opt.vocab_file)\n",
    "    n_docs = len(docs)\n",
    "    n_terms = len(vocab)\n",
    "    print('n_docs={}, n_terms={}'.format(n_docs, n_terms))\n",
    "\n",
    "    dt_mat = np.zeros([n_terms, n_terms])\n",
    "    for itm in docs:\n",
    "        for kk in itm:\n",
    "            for jj in itm:\n",
    "                if kk != jj:\n",
    "                    dt_mat[int(kk), int(jj)] += 1.0\n",
    "    print('co-occur done')\n",
    "\n",
    "    W = np.loadtxt(opt.par_file, dtype=float)\n",
    "    n_topic = W.shape[1]\n",
    "    print('n_topic={}'.format(n_topic))\n",
    "\n",
    "    PMI_arr = []\n",
    "    n_topKeyword = 10\n",
    "    for k in range(n_topic):\n",
    "        topKeywordsIndex = W[:,k].argsort()[::-1][:n_topKeyword]\n",
    "        PMI_arr.append(calculate_PMI(dt_mat, topKeywordsIndex))\n",
    "    print('Average PMI={}'.format(np.average(np.array(PMI_arr))))\n",
    "\n",
    "    index = np.argsort(PMI_arr)\n",
    "\n",
    "    for k in index:\n",
    "        print('Topic ' + str(k+1) + ': ', end=' ')\n",
    "        Topic_temp=k+1\n",
    "        print(PMI_arr[k], end=' ')\n",
    "        PMI_temp=PMI_arr[k]\n",
    "\n",
    "        for w in np.argsort(W[:,k])[::-1][:n_topKeyword]:\n",
    "            print(vocab[w], end=' ')\n",
    "        print()\n",
    "        \n",
    "    block=pd.DataFrame([]) #initialsing empty \n",
    "    val_temp=[] \n",
    "    PMI_temp=[]\n",
    "    Topic_temp=[]\n",
    "        \n",
    "    for k in index:\n",
    "\n",
    "        Topic_temp.append(k+1) #add 1 to value of topic number for acccuracy \n",
    "\n",
    "        PMI_temp.append(PMI_arr[k])# append PMI values to list \n",
    "\n",
    "        ar_temp=[] # initialise temporary list vocabulary to for each topic  \n",
    "\n",
    "        for w in np.argsort(W[:,k])[::-1][:n_topKeyword]: #iterating through each topic word \n",
    "            ar_temp.append(vocab[w]) #appending each word in topic to list \n",
    "\n",
    "        ar_join= ' '.join(ar_temp) #joining all separate words of topic into a single string \n",
    "\n",
    "        val_temp.append(ar_join) #appending final completed string into topic list \n",
    "    \n",
    "    block =block.append(pd.DataFrame({'Topic_No':Topic_temp,'PMI_value':PMI_temp,'Topic_string':val_temp}),ignore_index=True) #appending completed topic into a dataframe row \n",
    "\n",
    "    block.to_excel(results_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read documents\n",
      "--------------------------------------------------\n",
      "read vocabulary\n",
      "--------------------------------------------------\n",
      "n_docs=37996, n_terms=5999\n",
      "co-occur done\n",
      "n_topic=10\n",
      "Average PMI=0.9197677994026302\n",
      "Topic 4:  0.13156471390206848 plan council urge call water job back get report cut \n",
      "Topic 10:  0.5572487632731067 new kill crash car die dead swine_flu hospital law road \n",
      "Topic 8:  0.5715105838660369 govt urge act qld nt defend vic budget fund nsw \n",
      "Topic 9:  0.6111200852497305 man charge face die kill crash miss court murder attack \n",
      "Topic 3:  0.8658347545850102 say talk speak nrl_interview tell afl_interview extended_interview minister peter chief \n",
      "Topic 1:  0.869598984304244 police probe search miss investigate hunt find suspect death car \n",
      "Topic 5:  0.9761847721130219 win aussie lead top take star victory australia award return \n",
      "Topic 2:  0.9763791041406219 economic rudd demand economy china mining global trade government bank \n",
      "Topic 6:  1.4270489024905233 charge court accuse murder jail face allege woman teen assault \n",
      "Topic 7:  2.211187330101938 final fire nadal federer reach hewitt djokovic title semi open \n"
     ]
    }
   ],
   "source": [
    "NMF_Printout_topics('data/doc_term_mat_abcnews.txt','data/vocab.txt','seanmf_results/W.txt','text10.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion of topics into a dataframe for storage as a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "block=pd.DataFrame([]) #initialsing empty \n",
    "val_temp=[] \n",
    "PMI_temp=[]\n",
    "Topic_temp=[]\n",
    "\n",
    "\n",
    "for k in index:\n",
    "\n",
    "    Topic_temp.append(k+1) #add 1 to value of topic number for acccuracy \n",
    "    \n",
    "    PMI_temp.append(PMI_arr[k])# append PMI values to list \n",
    "     \n",
    "    ar_temp=[] # initialise temporary list vocabulary to for each topic  \n",
    "    \n",
    "    for w in np.argsort(W[:,k])[::-1][:n_topKeyword]: #iterating through each topic word \n",
    "        ar_temp.append(vocab[w]) #appending each word in topic to list \n",
    "    \n",
    "    ar_join= ' '.join(ar_temp) #joining all separate words of topic into a single string \n",
    "    \n",
    "    val_temp.append(ar_join) #appending final completed string into topic list \n",
    "    \n",
    "block =block.append(pd.DataFrame({'Topic_No':Topic_temp,'PMI_value':PMI_temp,'Topic_string':val_temp}),ignore_index=True) #appending completed topic into a dataframe row \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "block.to_excel('2009jan_2_jun_topics_with_text_preprocessing_300_topics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMI_value</th>\n",
       "      <th>Topic_No</th>\n",
       "      <th>Topic_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>289</td>\n",
       "      <td>distract panic dampen fielding harm deter mand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.476488</td>\n",
       "      <td>261</td>\n",
       "      <td>aged_care bega facility karumba desal recyclin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.486204</td>\n",
       "      <td>101</td>\n",
       "      <td>say time sorry goodbye expert afl_interview ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600738</td>\n",
       "      <td>6</td>\n",
       "      <td>new appoint zealand unveil facility guideline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.603865</td>\n",
       "      <td>84</td>\n",
       "      <td>may hurt park underage optus unknown playgroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.681749</td>\n",
       "      <td>178</td>\n",
       "      <td>fight vow step save kiwi mundine cambodia way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.704586</td>\n",
       "      <td>42</td>\n",
       "      <td>call origin_media spark opposition inquiry act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.705059</td>\n",
       "      <td>22</td>\n",
       "      <td>take toll control time responsibility stand st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.738871</td>\n",
       "      <td>286</td>\n",
       "      <td>nt scrymgour welfare croc chamber ant system p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.755950</td>\n",
       "      <td>24</td>\n",
       "      <td>govt feed ignore pressure must spend trouble e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.763607</td>\n",
       "      <td>181</td>\n",
       "      <td>back bounce operating burning bring controvers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.825737</td>\n",
       "      <td>191</td>\n",
       "      <td>school parent funding bus stimulus enrolment p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.825847</td>\n",
       "      <td>141</td>\n",
       "      <td>delay et inquiry baggage terminal removal vote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.833698</td>\n",
       "      <td>197</td>\n",
       "      <td>report ask rihanna release gibson abuse max da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.835958</td>\n",
       "      <td>55</td>\n",
       "      <td>face child_sex uncertain_future extinction han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.840307</td>\n",
       "      <td>4</td>\n",
       "      <td>health department system rebate dept director ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.846131</td>\n",
       "      <td>114</td>\n",
       "      <td>want mayor explanation prosecutor copenhagen r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.881736</td>\n",
       "      <td>20</td>\n",
       "      <td>sydney festival hma international film_festiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.882860</td>\n",
       "      <td>255</td>\n",
       "      <td>announce state henderson beazley ree retiremen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>300</td>\n",
       "      <td>nsw area northern govts ad coast opposition op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.892944</td>\n",
       "      <td>78</td>\n",
       "      <td>tas championship schoolboy muscle athlete incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.895950</td>\n",
       "      <td>253</td>\n",
       "      <td>claim dispute innings inaugural hamilton_smith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.908829</td>\n",
       "      <td>134</td>\n",
       "      <td>make comeback debut history sense friendly tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.918068</td>\n",
       "      <td>179</td>\n",
       "      <td>child protection abuse care catch irish pic un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.927803</td>\n",
       "      <td>291</td>\n",
       "      <td>hit hard low earthquake quake financial_crisis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.953791</td>\n",
       "      <td>96</td>\n",
       "      <td>close gap beach door yeppoon spot commitment m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.959713</td>\n",
       "      <td>174</td>\n",
       "      <td>power station solar outage western blackout li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.971160</td>\n",
       "      <td>148</td>\n",
       "      <td>job slash save ax safe mining creation promise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.984971</td>\n",
       "      <td>202</td>\n",
       "      <td>fall short boy hurt wine wall_st accident balc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.990704</td>\n",
       "      <td>220</td>\n",
       "      <td>set date alight trap pace sail meeting balibo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2.639638</td>\n",
       "      <td>46</td>\n",
       "      <td>mark celebrate anniversary th_anniversary cele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2.652528</td>\n",
       "      <td>31</td>\n",
       "      <td>oscar star award film slumdog honour ledger go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2.654247</td>\n",
       "      <td>283</td>\n",
       "      <td>rare auction fetch guitar endanger parrot roy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2.666541</td>\n",
       "      <td>274</td>\n",
       "      <td>election vote poll result frome contest early ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2.731429</td>\n",
       "      <td>110</td>\n",
       "      <td>president elect coup guinea african bissau mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2.740817</td>\n",
       "      <td>117</td>\n",
       "      <td>england windie pietersen flintoff ash strauss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2.760086</td>\n",
       "      <td>49</td>\n",
       "      <td>rebel troop sri_lanka sri_lankan defeat surren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2.775273</td>\n",
       "      <td>107</td>\n",
       "      <td>protea pont scg hussey clarke bat ton katich j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2.785189</td>\n",
       "      <td>139</td>\n",
       "      <td>great scientist reef barrier coral decline gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2.830726</td>\n",
       "      <td>44</td>\n",
       "      <td>hearing patel nurse whistleblower front testif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2.858290</td>\n",
       "      <td>199</td>\n",
       "      <td>success spirit lightning wnbl enjoy sing scot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>2.885577</td>\n",
       "      <td>269</td>\n",
       "      <td>gaza israel israeli hama ceasefire rocket offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>2.917044</td>\n",
       "      <td>176</td>\n",
       "      <td>red brumbie edge highlander match waratah supe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>3.061516</td>\n",
       "      <td>246</td>\n",
       "      <td>use fence picker cherry detainee remove materi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>3.082179</td>\n",
       "      <td>92</td>\n",
       "      <td>market local_market share gain wall_st stock b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>3.111500</td>\n",
       "      <td>236</td>\n",
       "      <td>join woody allen hood robin film nicole crowe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>3.118661</td>\n",
       "      <td>60</td>\n",
       "      <td>thief steal shop burglar pizza handed empty je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>3.138348</td>\n",
       "      <td>270</td>\n",
       "      <td>jackson fan music pop king mr mourn rip chart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>3.175221</td>\n",
       "      <td>32</td>\n",
       "      <td>gas russia europe ukraine supply resume russia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>3.216108</td>\n",
       "      <td>88</td>\n",
       "      <td>ship pirate cargo hijack somalia somali_pirat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>3.225284</td>\n",
       "      <td>34</td>\n",
       "      <td>brisbane priest sack catholic church archbisho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>3.252365</td>\n",
       "      <td>228</td>\n",
       "      <td>ahead sizzle perry surge ogilvy highland touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>3.293521</td>\n",
       "      <td>225</td>\n",
       "      <td>fly base fox raaf havoc wreak tindal hunter ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>3.332497</td>\n",
       "      <td>250</td>\n",
       "      <td>amateur ancient unearth coin archaeologist sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>3.381504</td>\n",
       "      <td>272</td>\n",
       "      <td>interview full shane listen glen brad sim palm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>3.437292</td>\n",
       "      <td>95</td>\n",
       "      <td>lawyer von einem porn read page admit phone ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>3.460951</td>\n",
       "      <td>11</td>\n",
       "      <td>final reach oust advance serena san upset jank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>3.546391</td>\n",
       "      <td>243</td>\n",
       "      <td>navy sri military lankas flee craft sink suici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>3.725227</td>\n",
       "      <td>213</td>\n",
       "      <td>pole gp button rossi motogp stoner clinch braw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>4.096508</td>\n",
       "      <td>106</td>\n",
       "      <td>nadal federer djokovic semi murray hewitt book...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PMI_value  Topic_No                                       Topic_string\n",
       "0     0.000000       289  distract panic dampen fielding harm deter mand...\n",
       "1     0.476488       261  aged_care bega facility karumba desal recyclin...\n",
       "2     0.486204       101  say time sorry goodbye expert afl_interview ca...\n",
       "3     0.600738         6  new appoint zealand unveil facility guideline ...\n",
       "4     0.603865        84  may hurt park underage optus unknown playgroun...\n",
       "5     0.681749       178  fight vow step save kiwi mundine cambodia way ...\n",
       "6     0.704586        42  call origin_media spark opposition inquiry act...\n",
       "7     0.705059        22  take toll control time responsibility stand st...\n",
       "8     0.738871       286  nt scrymgour welfare croc chamber ant system p...\n",
       "9     0.755950        24  govt feed ignore pressure must spend trouble e...\n",
       "10    0.763607       181  back bounce operating burning bring controvers...\n",
       "11    0.825737       191  school parent funding bus stimulus enrolment p...\n",
       "12    0.825847       141  delay et inquiry baggage terminal removal vote...\n",
       "13    0.833698       197  report ask rihanna release gibson abuse max da...\n",
       "14    0.835958        55  face child_sex uncertain_future extinction han...\n",
       "15    0.840307         4  health department system rebate dept director ...\n",
       "16    0.846131       114  want mayor explanation prosecutor copenhagen r...\n",
       "17    0.881736        20  sydney festival hma international film_festiva...\n",
       "18    0.882860       255  announce state henderson beazley ree retiremen...\n",
       "19    0.888889       300  nsw area northern govts ad coast opposition op...\n",
       "20    0.892944        78  tas championship schoolboy muscle athlete incl...\n",
       "21    0.895950       253  claim dispute innings inaugural hamilton_smith...\n",
       "22    0.908829       134  make comeback debut history sense friendly tri...\n",
       "23    0.918068       179  child protection abuse care catch irish pic un...\n",
       "24    0.927803       291  hit hard low earthquake quake financial_crisis...\n",
       "25    0.953791        96  close gap beach door yeppoon spot commitment m...\n",
       "26    0.959713       174  power station solar outage western blackout li...\n",
       "27    0.971160       148  job slash save ax safe mining creation promise...\n",
       "28    0.984971       202  fall short boy hurt wine wall_st accident balc...\n",
       "29    0.990704       220  set date alight trap pace sail meeting balibo ...\n",
       "..         ...       ...                                                ...\n",
       "270   2.639638        46  mark celebrate anniversary th_anniversary cele...\n",
       "271   2.652528        31  oscar star award film slumdog honour ledger go...\n",
       "272   2.654247       283  rare auction fetch guitar endanger parrot roy ...\n",
       "273   2.666541       274  election vote poll result frome contest early ...\n",
       "274   2.731429       110  president elect coup guinea african bissau mad...\n",
       "275   2.740817       117  england windie pietersen flintoff ash strauss ...\n",
       "276   2.760086        49  rebel troop sri_lanka sri_lankan defeat surren...\n",
       "277   2.775273       107  protea pont scg hussey clarke bat ton katich j...\n",
       "278   2.785189       139  great scientist reef barrier coral decline gro...\n",
       "279   2.830726        44  hearing patel nurse whistleblower front testif...\n",
       "280   2.858290       199  success spirit lightning wnbl enjoy sing scot ...\n",
       "281   2.885577       269  gaza israel israeli hama ceasefire rocket offe...\n",
       "282   2.917044       176  red brumbie edge highlander match waratah supe...\n",
       "283   3.061516       246  use fence picker cherry detainee remove materi...\n",
       "284   3.082179        92  market local_market share gain wall_st stock b...\n",
       "285   3.111500       236  join woody allen hood robin film nicole crowe ...\n",
       "286   3.118661        60  thief steal shop burglar pizza handed empty je...\n",
       "287   3.138348       270  jackson fan music pop king mr mourn rip chart ...\n",
       "288   3.175221        32  gas russia europe ukraine supply resume russia...\n",
       "289   3.216108        88  ship pirate cargo hijack somalia somali_pirat ...\n",
       "290   3.225284        34  brisbane priest sack catholic church archbisho...\n",
       "291   3.252365       228  ahead sizzle perry surge ogilvy highland touch...\n",
       "292   3.293521       225  fly base fox raaf havoc wreak tindal hunter ho...\n",
       "293   3.332497       250  amateur ancient unearth coin archaeologist sta...\n",
       "294   3.381504       272  interview full shane listen glen brad sim palm...\n",
       "295   3.437292        95  lawyer von einem porn read page admit phone ce...\n",
       "296   3.460951        11  final reach oust advance serena san upset jank...\n",
       "297   3.546391       243  navy sri military lankas flee craft sink suici...\n",
       "298   3.725227       213  pole gp button rossi motogp stoner clinch braw...\n",
       "299   4.096508       106  nadal federer djokovic semi murray hewitt book...\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
